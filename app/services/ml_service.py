"""
ML Prediction Service with LAZY LOADING & REMOTE MODEL FETCHING
Enhanced with: SHAP Explanation, N-gram Analysis, Keyword Detection
"""
import os
import re
from typing import List, Dict, Any, Optional
from collections import Counter
# [QUAN TRá»ŒNG] Import thÆ° viá»‡n Ä‘á»ƒ táº£i model tá»« kho riÃªng
from huggingface_hub import hf_hub_download

# Only set HF cache for local development
# if not os.getenv("RENDER") and not os.getenv("SPACE_ID"):
#     os.environ['HF_HOME'] = 'G:/huggingface_cache'


class KeywordAnalyzer:
    """Analyzes text for positive/negative keywords"""
    
    def __init__(self):
        # Vietnamese positive keywords
        self.positive_words = [
            'tá»‘t', 'Ä‘áº¹p', 'tuyá»‡t vá»i', 'xuáº¥t sáº¯c', 'hoÃ n háº£o', 'cháº¥t lÆ°á»£ng',
            'nhanh', 'tiá»‡n', 'Æ°ng', 'hÃ i lÃ²ng', 'thÃ­ch', 'yÃªu', 'tuyá»‡t',
            'ok', 'á»•n', 'Ä‘Æ°á»£c', 'giá»i', 'hay', 'ngon', 'xá»‹n', 'Ä‘á»‰nh',
            'pro', 'amazing', 'perfect', 'good', 'great', 'excellent',
            'ráº»', 'Ä‘Ã¡ng tiá»n', 'Ä‘Ã¡ng mua', 'recommend', 'khuyÃªn', 'nÃªn mua',
            'chÃ­nh hÃ£ng', 'uy tÃ­n', 'nhiá»‡t tÃ¬nh', 'chu Ä‘Ã¡o', 'cáº£m Æ¡n',
            'giao nhanh', 'Ä‘Ã³ng gÃ³i cáº©n tháº­n', 'Ä‘Ãºng mÃ´ táº£', 'nhÆ° hÃ¬nh',
            'ráº¥t tá»‘t', 'ráº¥t Ä‘áº¹p', 'ráº¥t Æ°ng', 'ráº¥t thÃ­ch', 'siÃªu', 'quÃ¡ Ä‘áº¹p'
        ]
        
        # Vietnamese negative keywords
        self.negative_words = [
            'tá»‡', 'xáº¥u', 'kÃ©m', 'dá»Ÿ', 'tá»“i', 'tháº¥t vá»ng', 'chÃ¡n',
            'cháº­m', 'lÃ¢u', 'lá»—i', 'há»ng', 'vá»¡', 'rÃ¡ch', 'báº©n',
            'giáº£', 'fake', 'lá»«a', 'Ä‘áº¯t', 'khÃ´ng Ä‘Ã¡ng', 'phÃ­ tiá»n',
            'bad', 'poor', 'terrible', 'awful', 'worst', 'horrible',
            'khÃ´ng thÃ­ch', 'khÃ´ng Æ°ng', 'khÃ´ng hÃ i lÃ²ng', 'khÃ´ng nhÆ°',
            'tráº£ láº¡i', 'hoÃ n tiá»n', 'khÃ´ng Ä‘Ãºng', 'sai', 'thiáº¿u',
            'giao cháº­m', 'Ä‘Ã³ng gÃ³i áº©u', 'mÃ³p', 'mÃ©o', 'cÅ©', 'ráº¥t tá»‡',
            'quÃ¡ tá»‡', 'khÃ´ng tá»‘t', 'khÃ´ng ok', 'dá»Ÿ áº¹t', 'ráº¥t xáº¥u'
        ]
    
    def analyze(self, text: str) -> Dict[str, Any]:
        """Analyze text for positive/negative keywords"""
        text_lower = text.lower()
        
        found_positive = []
        found_negative = []
        
        for word in self.positive_words:
            if word.lower() in text_lower:
                found_positive.append(word)
        
        for word in self.negative_words:
            if word.lower() in text_lower:
                found_negative.append(word)
        
        return {
            'positive_keywords': found_positive,
            'negative_keywords': found_negative,
            'positive_count': len(found_positive),
            'negative_count': len(found_negative)
        }


class NgramAnalyzer:
    """Analyzes text for n-grams"""
    
    def __init__(self):
        # Vietnamese stopwords to exclude
        self.stopwords = set([
            'vÃ ', 'cá»§a', 'cÃ³', 'cho', 'vá»›i', 'tá»«', 'nÃ y', 'Ä‘Æ°á»£c',
            'lÃ ', 'Ä‘á»ƒ', 'má»™t', 'cÃ¡c', 'trong', 'khÃ´ng', 'Ä‘Ã£', 'ráº¥t',
            'cÅ©ng', 'nhÆ°ng', 'thÃ¬', 'bá»‹', 'khi', 'náº¿u', 'nhÆ°', 'vá»',
            'tÃ´i', 'báº¡n', 'mÃ¬nh', 'nÃ³', 'há»', 'em', 'anh', 'chá»‹',
            'vÃ¬', 'nÃªn', 'Ä‘áº¿n', 'láº¡i', 'ra', 'Ä‘ang', 'sáº½', 'Ä‘á»u',
            'hay', 'tháº¿', 'lÃ m', 'rá»“i', 'Ä‘Ã³', 'á»Ÿ', 'tháº¥y', 'cÃ²n',
            'shop', 'sp', 'sáº£n pháº©m', 'hÃ ng', 'Ä‘Æ¡n', 'giao'
        ])
    
    def extract_ngrams(self, texts: List[str], n: int = 2, top_k: int = 15) -> List[Dict[str, Any]]:
        """Extract top n-grams from list of texts"""
        all_ngrams = []
        
        for text in texts:
            # Tokenize
            words = self._tokenize(text)
            # Filter stopwords
            words = [w for w in words if w.lower() not in self.stopwords and len(w) > 1]
            
            # Generate n-grams
            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i+n])
                all_ngrams.append(ngram)
        
        # Count and get top k
        counter = Counter(all_ngrams)
        top_ngrams = counter.most_common(top_k)
        
        return [{'ngram': ngram, 'count': count} for ngram, count in top_ngrams]
    
    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization for Vietnamese"""
        # Remove special characters but keep Vietnamese diacritics
        text = re.sub(r'[^\w\sÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘]', ' ', text.lower())
        return text.split()
    
    def analyze_single(self, text: str) -> Dict[str, List[Dict[str, Any]]]:
        """Analyze single text for unigrams, bigrams, trigrams"""
        return {
            'unigrams': self.extract_ngrams([text], n=1, top_k=10),
            'bigrams': self.extract_ngrams([text], n=2, top_k=10),
            'trigrams': self.extract_ngrams([text], n=3, top_k=10)
        }
    
    def analyze_batch(self, texts: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """Analyze batch of texts for n-grams"""
        return {
            'unigrams': self.extract_ngrams(texts, n=1, top_k=15),
            'bigrams': self.extract_ngrams(texts, n=2, top_k=15),
            'trigrams': self.extract_ngrams(texts, n=3, top_k=10)
        }


class MLPredictionService:
    """
    ML Service with lazy loading.
    Fetches heavy model weights from external Hugging Face Model Repo
    to bypass the 1GB limit of Space Git Repo.
    """

    def __init__(self):
        """Initialize service without loading model (lazy loading)"""
        # Model components
        self.model: Optional[Any] = None
        self.tokenizer: Optional[Any] = None
        self.device: Optional[str] = None
        self.model_loaded = False
        
        # [Sá»¬A Äá»”I] KhÃ´ng set Ä‘Æ°á»ng dáº«n cá»©ng á»Ÿ Ä‘Ã¢y ná»¯a vÃ¬ file khÃ´ng cÃ²n á»Ÿ mÃ¡y
        # ChÃºng ta sáº½ Ä‘á»‹nh nghÄ©a Repo ID chá»©a model á»Ÿ Ä‘Ã¢y
        self.MODEL_REPO_ID = "vtdung23/my-phobert-models"
        self.MODEL_FILENAME = "best_phoBER.pth"
        
        # Initialize analyzers
        self.keyword_analyzer = KeywordAnalyzer()
        self.ngram_analyzer = NgramAnalyzer()
        
        print("âœ… ML Service initialized (Model will download & load on first request)")

    
    def _load_model(self):
        """Load model and tokenizer (called on first request)"""
        if self.model_loaded:
            return
        
        print("ðŸ”„ Loading ML model (first request)...")
        
        # Import heavy dependencies only when needed
        import torch
        from transformers import AutoTokenizer, RobertaForSequenceClassification
        
        # Determine device
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ðŸ“ Using device: {self.device}")
        
        # [Sá»¬A Äá»”I 1] Load Tokenizer tá»« gá»‘c vinai/phobert-base
        # VÃ¬ folder tokenizer local Ä‘Ã£ bá»‹ xÃ³a, ta load tháº³ng tá»« thÆ° viá»‡n gá»‘c cho an toÃ n
        print("ðŸ“¦ Loading tokenizer from vinai/phobert-base...")
        self.tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base", use_fast=False)
        
        # [Sá»¬A Äá»”I 2] Táº£i file weights tá»« Kho Model riÃªng vá»
        print(f"â¬‡ï¸ Downloading weights from repo: {self.MODEL_REPO_ID}...")
        try:
            model_path = hf_hub_download(
                repo_id=self.MODEL_REPO_ID,
                filename=self.MODEL_FILENAME,
                repo_type="model" # Quan trá»ng: bÃ¡o Ä‘Ã¢y lÃ  kho Model
            )
            print(f"âœ… Downloaded weights to: {model_path}")
        except Exception as e:
            print(f"âŒ Error downloading model: {e}")
            raise e

        # Load model architecture
        print("ðŸ§  Loading PhoBERT architecture...")
        self.model = RobertaForSequenceClassification.from_pretrained(
            "vinai/phobert-base",
            num_labels=5, # Äáº£m báº£o sá»‘ nÃ y khá»›p vá»›i lÃºc báº¡n train (0,1,2,3,4 hay 1-5?)
            problem_type="single_label_classification"
        )
        
        # Load fine-tuned weights
        print("âš™ï¸ Loading trained weights into architecture...")
        state_dict = torch.load(model_path, map_location=self.device, weights_only=False)
        self.model.load_state_dict(state_dict)
        
        # Set to evaluation mode and move to device
        self.model.eval()
        self.model.to(self.device)
        
        self.model_loaded = True
        print("âœ… Model loaded successfully and ready to serve!")
            
    def predict_single(self, text: str) -> Dict[str, Any]:
        """Predict rating for a single comment"""
        # Lazy load model on first request
        self._load_model()
        
        import torch
        import torch.nn.functional as F

        # 1. Vietnamese preprocessing
        processed_text = self.preprocess(text)

        # 2. Tokenize
        encoded = self.tokenizer(
            processed_text,
            padding=True,
            truncation=True,
            max_length=256,
            return_tensors="pt"
        )
        
        # Move tensors to device
        encoded = {k: v.to(self.device) for k, v in encoded.items()}

        # 3. Inference
        with torch.no_grad():
            outputs = self.model(**encoded)
            logits = outputs.logits
            probs = F.softmax(logits, dim=1)

        # 4. Get prediction + confidence
        predicted_class = torch.argmax(probs, dim=1).item()
        confidence = probs[0][predicted_class].item()

        # 5. Convert 0-based label -> rating 1-5
        # (Giáº£ sá»­ model train label 0 tÆ°Æ¡ng á»©ng 1 sao)
        rating = predicted_class + 1

        return {
            'rating': rating,
            'confidence': confidence
        }
    
    def predict_with_explanation(self, text: str) -> Dict[str, Any]:
        """
        Predict rating with explanation (word importance scores)
        Uses gradient-based attribution for interpretability
        """
        # Lazy load model on first request
        self._load_model()
        
        import torch
        import torch.nn.functional as F
        
        # 1. Vietnamese preprocessing
        processed_text = self.preprocess(text)
        words = processed_text.split()
        
        # 2. Tokenize
        encoded = self.tokenizer(
            processed_text,
            padding=True,
            truncation=True,
            max_length=256,
            return_tensors="pt"
        )
        
        # Move tensors to device
        encoded = {k: v.to(self.device) for k, v in encoded.items()}
        
        # 3. Get embeddings and enable gradient computation
        embeddings = self.model.roberta.embeddings(encoded['input_ids'])
        embeddings.requires_grad_(True)
        
        # 4. Forward pass with embeddings
        with torch.enable_grad():
            outputs = self.model.roberta.encoder(embeddings)
            sequence_output = outputs.last_hidden_state
            logits = self.model.classifier(sequence_output)
            probs = F.softmax(logits, dim=1)
            
            # Get predicted class
            predicted_class = torch.argmax(probs, dim=1).item()
            confidence = probs[0][predicted_class].item()
            
            # Compute gradient for the predicted class
            target_score = probs[0][predicted_class]
            target_score.backward()
            
            # Get gradient-based importance
            gradients = embeddings.grad
            importance = gradients.abs().sum(dim=-1).squeeze()
        
        # 5. Map importance to words (simplified - using tokenizer alignment)
        tokens = self.tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])
        token_importance = importance.detach().cpu().numpy()
        
        # Normalize importance scores
        if token_importance.max() > 0:
            token_importance = token_importance / token_importance.max()
        
        # Map to original words (simplified approach)
        word_importance = []
        for i, token in enumerate(tokens):
            if token not in ['<s>', '</s>', '<pad>']:
                # Assign positive/negative based on keyword analysis
                keyword_analysis = self.keyword_analyzer.analyze(token)
                base_score = float(token_importance[i])
                
                if keyword_analysis['positive_count'] > 0:
                    word_importance.append({
                        'word': token,
                        'score': base_score  # Positive contribution
                    })
                elif keyword_analysis['negative_count'] > 0:
                    word_importance.append({
                        'word': token,
                        'score': -base_score  # Negative contribution
                    })
                else:
                    word_importance.append({
                        'word': token,
                        'score': base_score * (0.5 if predicted_class >= 2 else -0.5)
                    })
        
        rating = predicted_class + 1
        
        # Also get keyword analysis for the full text
        keyword_analysis = self.keyword_analyzer.analyze(text)
        
        return {
            'rating': rating,
            'confidence': confidence,
            'explanation': {
                'words': [wi['word'] for wi in word_importance[:20]],
                'importance_scores': [wi['score'] for wi in word_importance[:20]],
                'overall_sentiment': 'positive' if rating >= 4 else ('negative' if rating <= 2 else 'neutral')
            },
            'keywords': keyword_analysis
        }
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, any]]:
        """Predict ratings for multiple comments"""
        results = []
        for text in texts:
            # CÃ³ thá»ƒ tá»‘i Æ°u báº±ng cÃ¡ch batch tokenize, nhÆ°ng loop Ä‘Æ¡n giáº£n cho an toÃ n
            prediction = self.predict_single(text)
            results.append({
                'text': text,
                'rating': prediction['rating'],
                'confidence': prediction['confidence']
            })
        return results
    
    def predict_batch_with_analysis(self, texts: List[str]) -> Dict[str, Any]:
        """
        Predict ratings for batch with additional analysis:
        - N-gram analysis
        - Keyword frequency
        - Rating distribution
        """
        # Get predictions
        predictions = self.predict_batch(texts)
        
        # N-gram analysis
        ngram_analysis = self.ngram_analyzer.analyze_batch(texts)
        
        # Aggregate keyword analysis
        all_positive = []
        all_negative = []
        for text in texts:
            kw = self.keyword_analyzer.analyze(text)
            all_positive.extend(kw['positive_keywords'])
            all_negative.extend(kw['negative_keywords'])
        
        positive_freq = Counter(all_positive).most_common(10)
        negative_freq = Counter(all_negative).most_common(10)
        
        return {
            'predictions': predictions,
            'ngrams': ngram_analysis,
            'keyword_frequency': {
                'positive': [{'word': w, 'count': c} for w, c in positive_freq],
                'negative': [{'word': w, 'count': c} for w, c in negative_freq]
            }
        }
    
    def analyze_ngrams(self, texts: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """Analyze n-grams for a list of texts"""
        return self.ngram_analyzer.analyze_batch(texts)
    
    def preprocess(self, text: str) -> str:
        """Preprocess Vietnamese text"""
        from underthesea import word_tokenize
        text = word_tokenize(text, format="text")
        return text

# Singleton instance
ml_service = MLPredictionService()

def get_ml_service() -> MLPredictionService:
    return ml_service